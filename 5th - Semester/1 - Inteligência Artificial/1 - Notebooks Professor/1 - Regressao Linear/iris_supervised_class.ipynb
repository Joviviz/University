{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T22:59:25.780750Z",
     "start_time": "2024-08-11T22:59:24.011443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ],
   "id": "e0f1e77e2bf488ff",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T22:59:28.619250Z",
     "start_time": "2024-08-11T22:59:28.608975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Carregar o conjunto de dados Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Padronizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "id": "4e831344fe15b9d6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data Augmentation: Ruído gaussiano\n",
    "================\n",
    "Conjunto de técnicas que evitam Overfitting (hiper-ajuste aos dados de treinamento, ou seja, aprende-se os dados treinados, não as características do problema)."
   ],
   "id": "30e852b6c92d9417"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "O que é Ruído Gaussiano?\n",
    "_______________\n",
    "\n",
    "* Gerado a partir de uma distribuição normal, caracterizada por uma média $\\mu$ e um desvio-padrão $\\sigma$;\n",
    "* O ruído deve ser pequeno o suficiente para não distorcer significativamente os dados, mas grandeo suficiente para criar variações úteis"
   ],
   "id": "678ce1bb7aa7e92f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adicão de ruído gaussiano\n",
    "___________________________\n",
    "\n",
    "* Para cada ponto de dados original, crie uma nova amostra adicionando um ruído gaussiano. A nova amostra será:\n",
    "\n",
    "$$ X_novo = X_original + \\mathcal{E}$$\n",
    "\n",
    "Onde o ruído gaussiano $\\mathcal{E}$:\n",
    "\n",
    "* $\\mathcal{E} \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "* $\\sigma$ deve ser escolhido com base na escala e na sensibilidade dos dados"
   ],
   "id": "94d189b1f293eb4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Escolha do valor adequado para $\\sigma$\n",
    "__________________________________________\n",
    "\n",
    "* Se o desvio padrão do ruído for muito pequeno, as novas amostras serão muito semelhantes aos dados originais, oferecendo pouca diversidade;\n",
    "* Se for muito grande, o ruído pode introduzir uma variabilidade que não está presente nos dados reais, potencialmente corrompendo a integridade dos dados;\n",
    "* Em geral, um bom ponto de partida para o desvio padrão do ruído é cerca de 1-5% da faixa (range) de cada feature."
   ],
   "id": "4b932019582ad81a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6d5535f85cf3c43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T22:59:37.375510Z",
     "start_time": "2024-08-11T22:59:37.369485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IrisClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisClassificationModel, self).__init__()\n",
    "        self.hidden = nn.Linear(X_train.shape[1], 16)  # Camada oculta com 16 neurônios\n",
    "        self.output = nn.Linear(16, 3)  # Três classes de saída\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))  # Função de ativação ReLU na camada oculta\n",
    "        x = self.output(x)  # Saída direta (softmax será aplicado na função de perda)\n",
    "        return x\n",
    "\n",
    "model = IrisClassificationModel()"
   ],
   "id": "abb9d1f0b21dc6f5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T22:59:53.007598Z",
     "start_time": "2024-08-11T22:59:52.059459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "id": "eb9d413a02f402e2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T22:59:58.301587Z",
     "start_time": "2024-08-11T22:59:55.900406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader_augmented:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass e otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Mostrar o progresso a cada 10 épocas\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ],
   "id": "93cbaddb11f9c547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.8353\n",
      "Epoch [20/100], Loss: 0.5800\n",
      "Epoch [30/100], Loss: 0.4165\n",
      "Epoch [40/100], Loss: 0.4703\n",
      "Epoch [50/100], Loss: 0.3460\n",
      "Epoch [60/100], Loss: 0.4388\n",
      "Epoch [70/100], Loss: 0.3664\n",
      "Epoch [80/100], Loss: 0.3571\n",
      "Epoch [90/100], Loss: 0.2510\n",
      "Epoch [100/100], Loss: 0.1780\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T23:00:47.227446Z",
     "start_time": "2024-08-11T23:00:47.220798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converter os dados de teste para tensores do PyTorch\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Avaliar o modelo no conjunto de teste\n",
    "model.eval()  # Colocar o modelo em modo de avaliação\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n"
   ],
   "id": "4b863a7283bfda86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 96.67%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data Augmentation: Mixup\n",
    "================================\n",
    "\n",
    "técnica que cria novas amostras de treinamento ao combinar pares de amostras existentes, tanto em termos de features quanto de rótulos. Essa técnica é particularmente eficaz em melhorar a robustez e a generalização do modelo, especialmente em tarefas de classificação."
   ],
   "id": "e23a0a4e4685e26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Fórmulas do Mixup\n",
    "_____________________\n",
    "A ideia central do Mixup é criar novas amostras lineares interpolando entre dois exemplos aleatórios do conjunto de dados.\n",
    "* Para duas amostras $(x_i , y_i)$ e $(x_j , y_j)$, uma nova amostra $(\\tilde{x}, \\tilde{y})$ é criada da seguinte forma:\n",
    "\n",
    "$$\\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j$$\n",
    "$$\\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j$$\n",
    "\n",
    "Onde $\\lambda$ é um valor retirado de uma distribuição beta $\\beta (\\alpha , \\alpha)$, com $\\alpha$ sendo um hiperparâmetro que contra a mistura"
   ],
   "id": "8e7952c8f16fc582"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f6e1ddeaaff881e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data Augmentation: SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "====================================================================\n",
    "\n",
    "Técnica de aumento de dados que gera novos exemplos sintéticos para a classe minoritária, com o objetivo de equilibrar o conjunto de dados."
   ],
   "id": "7cd2dd472b901f2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "301ea70fa6238032"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
