{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "FUNÇÕES DE ATIVAÇÃO\n",
    "===================\n",
    "As funções de ativação são responsáveis por determinar se um neurônio em uma rede neural deve ser ativado, ou seja, se deve transmitir o sinal adiante para a próxima camada. Elas introduzem não linearidade no modelo, o que é crucial para que a rede neural seja capaz de aprender padrões complexos. Sem essas funções, a rede seria simplesmente uma combinação linear, incapaz de resolver problemas complexos."
   ],
   "id": "851b9e002a7fc9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sigmóide\n",
    "----------\n",
    "* Valores de saída entre 0 e 1.\n",
    "* Utilizada em problemas de classificação binária.\n",
    "* Sofre com o problema de \"vanishing gradient\" em redes profundas, o que pode dificultar o treinamento.\n",
    "* $\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
   ],
   "id": "11d14beca6fca39a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tangente Hiperbólica\n",
    "---------------\n",
    "\n",
    "* Os valores de saída variam entre -1 e 1.\n",
    "* É mais \"balanceada\" que a sigmoide, já que a saída está centrada em torno de zero, mas também sofre com o problema de gradientes pequenos em valores extremos.\n",
    "* $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$"
   ],
   "id": "108505219d45d902"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ReLU (Rectified Linear Unit)\n",
    "----------------\n",
    "\n",
    "* Introduz não linearidade de uma forma muito simples e eficiente.\n",
    "* Facilita o cálculo, já que o gradiente é constante (1) para entradas positivas.\n",
    "* Um problema comum é o \"neurônio morto\", quando as entradas são muito negativas, levando a uma saída zero constante e impossibilidade de aprendizado.\n",
    "* $\\text{ReLU}(x) = \\max(0, x)$"
   ],
   "id": "97056b02d41da7c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Leaky ReLU\n",
    "-----------\n",
    "* É uma variante da ReLU, mas permite que valores negativos passem, com um pequeno gradiente, evitando o problema do \"neurônio morto\".\n",
    "* $\\text{Leaky ReLU}(x) = \\max(0.01x, x)$"
   ],
   "id": "a6d9824a1689ea4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Softmax\n",
    "----------\n",
    "\n",
    "* Normaliza a saída para que a soma de todas as probabilidades seja igual a 1.\n",
    "* Comumente usada na última camada de redes neurais para problemas de classificação multiclasse.\n",
    "* $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$"
   ],
   "id": "ef656c7a875be3c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "FUNÇÕES DE LOSS (PERDA)\n",
    "===================\n",
    "\n",
    "As funções de loss, ou funções de custo, são usadas para medir o erro ou discrepância entre as previsões do modelo e os valores reais dos dados de treinamento. O objetivo de qualquer modelo de aprendizado de máquina é minimizar essa função de perda durante o treinamento, ajustando os pesos da rede."
   ],
   "id": "f1c035a0119b0927"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "MSE - Mean Squared Error\n",
    "-----------------------\n",
    "\n",
    "* Utilizada em problemas de regressão.\n",
    "* Penaliza erros maiores de forma mais severa.\n",
    "* A função é suave e derivável, o que facilita o uso no treinamento via gradiente descendente.\n",
    "* $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$"
   ],
   "id": "9ca07bcf1adf4e53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "MAE - Mean Absolute Error\n",
    "--------------------\n",
    "* Outra função comum para problemas de regressão.\n",
    "* Penaliza os erros de forma linear, sendo mais robusta a outliers do que o MSE.\n",
    "* No entanto, não é derivável em $y_i = \\hat{y_i}$, o que pode dificultar a otimização;\n",
    "* $MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|$"
   ],
   "id": "f4efa5823c8ea88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "RMSE - Root Mean Squared Error\n",
    "--------------------\n",
    "\n",
    "* Expressa o erro na mesma unidade das variáveis preditas, facilitando a interpretação.\n",
    "* Quanto menor o valor do RMSE, melhor o modelo.\n",
    "* $RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}$"
   ],
   "id": "521bf9f5dfe8aa45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Cross-Entropy Loss\n",
    "------------------\n",
    "\n",
    "* Amplamente utilizada em problemas de classificação binária.\n",
    "* Penaliza severamente predições que estão longe das verdadeiras classes, forçando o modelo a aprender probabilidades corretas.\n",
    "* Para problemas de multiclasse, usa-se a variante com softmax na saída.\n",
    "* $L = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right]$"
   ],
   "id": "6a710d9f479345b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hinge Loss\n",
    "-----------\n",
    "\n",
    "* Utilizada em modelos de classificação, especialmente em máquinas de vetores de suporte (SVM).\n",
    "* É ideal para maximizar a margem entre as classes.\n",
    "* $L = \\sum_{i=1}^{n} \\max(0, 1 - y_i \\hat{y_i})$"
   ],
   "id": "61c1e8391f59196f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Huber Loss\n",
    "------------\n",
    "* É robusta a outliers, combinando as vantagens do MAE e do MSE.\n",
    "* Uma combinação entre MSE e MAE, usa o MSE para erros pequenos e MAE para erros grandes."
   ],
   "id": "4e8ce8072c1bc1d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Problemas de classificação binária\n",
    "============================\n",
    "\n",
    "**Exemplo:** Classificar se um e-mail é spam ou não.\n",
    "\n",
    "* **Entrada:** Texto do e-mail.\n",
    "* **Saída:** Uma probabilidade entre 0 e 1 indicando se o e-mail é spam (1) ou não (0).\n",
    "\n",
    "**Função de ativação:**\n",
    "\n",
    "* **Sigmoide** na última camada. A sigmoide transforma o valor em uma probabilidade entre 0 e 1, adequada para problemas binários.\n",
    "\n",
    "**Função de perda:**\n",
    "\n",
    "* **Cross-Entropy Loss (Entropia Cruzada Binária):** Esta função calcula a discrepância entre as probabilidades previstas e as classes reais (0 ou 1)."
   ],
   "id": "9adecd57b8a06440"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Definir o modelo\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=1000, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar o modelo com RMSE\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Exemplo de treinamento\n",
    "# model.fit(X_train, y_train, epochs=10)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassificationModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(1000, 16)\n",
    "        self.layer2 = nn.Linear(16, 8)\n",
    "        self.output = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.sigmoid(self.output(x))  # Sigmoid for binary classification\n",
    "        return x\n",
    "\n",
    "# Modelo, perda e otimizador\n",
    "model = BinaryClassificationModel()\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Função para calcular RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Exemplo de loop de treino\n",
    "# for epoch in range(num_epochs):\n",
    "#     outputs = model(inputs)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n"
   ],
   "id": "ee7966d20020661f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Problemas de classificação multiclasse\n",
    "============================\n",
    "\n",
    "**Exemplo:** Classificar o tipo de flor entre três categorias: 'setosa', 'versicolor' e 'virginica'.\n",
    "\n",
    "* **Entrada:** Comprimento e largura das pétalas e sépalas.\n",
    "* **Saída:** Uma das três classes possíveis (0, 1, 2).\n",
    "\n",
    "**Função de ativação:**\n",
    "\n",
    "* **Softmax** na última camada. O softmax transforma a saída em probabilidades que somam 1, adequadas para problemas multiclasse.\n",
    "\n",
    "**Função de perda:**\n",
    "\n",
    "* **Categorical Cross-Entropy (Entropia Cruzada Categórica):** Penaliza predições incorretas em relação às classes verdadeiras."
   ],
   "id": "29e409bb12ab88ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Modelo de classificação multiclasse\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=4, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Três classes\n",
    "])\n",
    "\n",
    "# Compilar o modelo com RMSE\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Exemplo de treinamento\n",
    "# model.fit(X_train, y_train, epochs=10)\n"
   ],
   "id": "e24a8ab5dff17541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MulticlassClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MulticlassClassificationModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 16)\n",
    "        self.layer2 = nn.Linear(16, 8)\n",
    "        self.output = nn.Linear(8, 3)  # Três classes na saída\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.softmax(self.output(x), dim=1)  # Softmax for multiclass classification\n",
    "        return x\n",
    "\n",
    "# Modelo, perda e otimizador\n",
    "model = MulticlassClassificationModel()\n",
    "criterion = nn.CrossEntropyLoss()  # Categorical Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Função para calcular RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ],
   "id": "11419ac546e24aff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "796545765485db4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelo de regressão\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=5, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='linear')  # Nenhuma ativação ou linear para regressão\n",
    "])\n",
    "\n",
    "# Compilar o modelo com RMSE\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Exemplo de treinamento\n",
    "# model.fit(X_train, y_train, epochs=10)\n"
   ],
   "id": "5bfa65a8b6be5802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(5, 16)\n",
    "        self.layer2 = nn.Linear(16, 8)\n",
    "        self.output = nn.Linear(8, 1)  # Saída com um único valor (preço da casa)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.output(x)  # Linear activation (nenhuma ativação)\n",
    "        return x\n",
    "\n",
    "# Modelo, perda e otimizador\n",
    "model = RegressionModel()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Função para calcular RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ],
   "id": "4880a734310a9fe6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Problema de Classificação Multilabel\n",
    "===============\n",
    "\n",
    "**Exemplo:** Classificar se uma imagem contém várias classes simultaneamente (por exemplo, 'gato', 'cachorro', 'pássaro' em uma imagem).\n",
    "\n",
    "* **Entrada:** Imagem com múltiplos objetos.\n",
    "* **Saída:** Um vetor de 0s e 1s indicando a presença ou ausência de cada classe.\n",
    "\n",
    "**Função de ativação:**\n",
    "\n",
    "* **Sigmoide** na última camada. Como cada classe é avaliada independentemente, a função sigmoide é usada para transformar a saída em probabilidades para cada classe.\n",
    "\n",
    "**Função de perda:**\n",
    "\n",
    "* **Binary Cross-Entropy (Entropia Cruzada Binária):** Esta função de perda é adequada para problemas multilabel onde cada classe é independente."
   ],
   "id": "85afa5c44450bcdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelo de classificação multilabel\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=1024, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='sigmoid')  # 10 classes independentes\n",
    "])\n",
    "\n",
    "# Compilar o modelo com RMSE\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Exemplo de treinamento\n",
    "# model.fit(X_train, y_train, epochs=10)\n"
   ],
   "id": "864cb5a91fe858cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MultilabelClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultilabelClassificationModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(1024, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10)  # 10 classes independentes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.sigmoid(self.output(x))  # Sigmoid for multilabel classification\n",
    "        return x\n",
    "\n",
    "# Modelo, perda e otimizador\n",
    "model = MultilabelClassificationModel()\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multilabel classification\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Função para calcular RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ],
   "id": "b659865edf44c9a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Problema de Regressão Robusta\n",
    "=====================\n",
    "\n",
    "**Exemplo:** Prever o valor de vendas, mas o conjunto de dados contém muitos outliers.\n",
    "\n",
    "* **Entrada:** Características que influenciam as vendas.\n",
    "* **Saída:** Um valor numérico contínuo (previsão de vendas).\n",
    "\n",
    "**Função de ativação:**\n",
    "\n",
    "* **Linear** na última camada.\n",
    "\n",
    "**Função de perda:**\n",
    "\n",
    "* **Huber Loss:** Combina o MSE para pequenos erros e o MAE para erros grandes, sendo mais robusta a outliers."
   ],
   "id": "d981a7b28f2f88a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelo de regressão robusta\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=10, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compilar o modelo com RMSE\n",
    "model.compile(optimizer=Adam(), loss=tf.keras.losses.Huber(), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Exemplo de treinamento\n",
    "# model.fit(X_train, y_train, epochs=10)\n"
   ],
   "id": "79b04b2aec03a07b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RobustRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobustRegressionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.output(x)  # Linear activation\n",
    "        return x\n",
    "\n",
    "# Modelo, perda e otimizador\n",
    "model = RobustRegressionModel()\n",
    "criterion = nn.SmoothL1Loss()  # Huber Loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Função para calcular RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ],
   "id": "419c1e34318b3e11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Problema com SVM (Hinge Loss)\n",
    "================\n",
    "\n",
    "**Exemplo:** Classificar se uma imagem contém um objeto específico ou não, usando uma Máquina de Vetores de Suporte (SVM).\n",
    "\n",
    "* **Entrada:** Vetor de características da imagem.\n",
    "* **Saída:** Uma classe binária (-1 ou 1), indicando a presença ou ausência do objeto.\n",
    "\n",
    "**Função de ativação:**\n",
    "\n",
    "* **Linear** ou sem ativação (SVMs geralmente não utilizam funções de ativação).\n",
    "\n",
    "**Função de perda:**\n",
    "\n",
    "* **Hinge Loss:** Penaliza predições incorretas com base em uma margem de separação entre as classes."
   ],
   "id": "f0dbb1a675c5ec4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelo de SVM usando Hinge Loss\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=10, activation='linear')  # Nenhuma ativação\n",
    "])\n",
    "\n",
    "# Compilar o modelo com Hinge Loss e RMSE\n",
    "model.compile(optimizer=Adam(), loss='hinge', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Exemplo de treinamento\n",
    "# model.fit(X_train, y_train, epochs=10)\n"
   ],
   "id": "b17cff256f99b604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SVMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVMModel, self).__init__()\n",
    "        self.layer = nn.Linear(10, 1)  # Camada linear única para simular SVM\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)  # Nenhuma ativação\n",
    "\n",
    "# Modelo, perda e otimizador\n",
    "model = SVMModel()\n",
    "criterion = nn.MultiMarginLoss()  # Hinge Loss approximation\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Função para calcular RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ],
   "id": "8316a6c29247d43e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "SVM - Support Vector Machine\n",
    "=================\n",
    "\n",
    "* Amplamente utilizados em aplicações como reconhecimento de imagens, bioinformática e detecção de fraudes devido à sua robustez e capacidade de lidar com dados de alta dimensionalidade.\n",
    "* Seu objetivo principal é encontrar um hiperplano que melhor separa os dados em diferentes classes. Aqui estão os principais conceitos:\n",
    "\n",
    "1. **Margem Máxima:** O SVM tenta encontrar o hiperplano que separa as classes de maneira mais ampla possível, maximizando a distância (margem) entre os pontos de dados mais próximos de cada classe, chamados de vetores de suporte.\n",
    "\n",
    "2. **Vetores de Suporte:** São os pontos de dados que estão mais próximos do hiperplano de separação e que influenciam sua posição. Esses pontos são críticos para definir o limite de decisão.\n",
    "\n",
    "3. **Classificação Linear:** Quando os dados são linearmente separáveis, o SVM encontra uma linha (no caso de 2D) ou um hiperplano (em dimensões mais altas) que separa claramente as duas classes.\n",
    "\n",
    "4. **Classificação Não Linear:** Para dados que não são linearmente separáveis, o SVM pode utilizar o truque do kernel (kernel trick), que transforma os dados para um espaço de maior dimensão onde eles podem ser separados linearmente. Os kernels mais usados são o polinomial e o RBF (Radial Basis Function).\n",
    "\n",
    "5. **Parâmetro C:** Controla o quanto o modelo tenta evitar classificações erradas dos dados. Um valor de C mais alto significa que o SVM tenta classificar todos os pontos corretamente, resultando em uma margem mais estreita, enquanto um C mais baixo permite uma margem mais ampla, com alguns pontos mal classificados.\n",
    "\n",
    "6. **Kernel:** Um dos principais pontos fortes do SVM. Ele permite que o SVM lide com problemas de classificação não linear, mapeando os dados para espaços de maior dimensão onde a separação linear pode ser possível."
   ],
   "id": "da0cc5d52449101d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
