{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:53:27.217034Z",
     "start_time": "2024-08-12T10:53:24.925495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ],
   "id": "e0f1e77e2bf488ff",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:53:28.662506Z",
     "start_time": "2024-08-12T10:53:28.652915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Carregar o conjunto de dados Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Padronizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "id": "4e831344fe15b9d6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data Augmentation: Ruído gaussiano\n",
    "================\n",
    "Conjunto de técnicas que evitam Overfitting (hiper-ajuste aos dados de treinamento, ou seja, aprende-se os dados treinados, não as características do problema)."
   ],
   "id": "30e852b6c92d9417"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "O que é Ruído Gaussiano?\n",
    "_______________\n",
    "\n",
    "* Gerado a partir de uma distribuição normal, caracterizada por uma média $\\mu$ e um desvio-padrão $\\sigma$;\n",
    "* O ruído deve ser pequeno o suficiente para não distorcer significativamente os dados, mas grandeo suficiente para criar variações úteis"
   ],
   "id": "678ce1bb7aa7e92f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adicão de ruído gaussiano\n",
    "___________________________\n",
    "\n",
    "* Para cada ponto de dados original, crie uma nova amostra adicionando um ruído gaussiano. A nova amostra será:\n",
    "\n",
    "$$ X_novo = X_original + \\mathcal{E}$$\n",
    "\n",
    "Onde o ruído gaussiano $\\mathcal{E}$:\n",
    "\n",
    "* $\\mathcal{E} \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "* $\\sigma$ deve ser escolhido com base na escala e na sensibilidade dos dados"
   ],
   "id": "94d189b1f293eb4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Escolha do valor adequado para $\\sigma$\n",
    "__________________________________________\n",
    "\n",
    "* Se o desvio padrão do ruído for muito pequeno, as novas amostras serão muito semelhantes aos dados originais, oferecendo pouca diversidade;\n",
    "* Se for muito grande, o ruído pode introduzir uma variabilidade que não está presente nos dados reais, potencialmente corrompendo a integridade dos dados;\n",
    "* Em geral, um bom ponto de partida para o desvio padrão do ruído é cerca de 1-5% da faixa (range) de cada feature."
   ],
   "id": "4b932019582ad81a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:55:55.042258Z",
     "start_time": "2024-08-12T10:55:55.036268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Função para adicionar ruído gaussiano\n",
    "def add_gaussian_noise(data, sigma=0.05):\n",
    "    noise = np.random.normal(0, sigma, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "# Adicionar ruído aos dados d2. ______________________e treino\n",
    "X_train_noisy = add_gaussian_noise(X_train, sigma=0.05)\n",
    "\n",
    "# Combinar os dados originais com os dados aumentados\n",
    "X_train_augmented = np.vstack((X_train, X_train_noisy))\n",
    "y_train_augmented = np.hstack((y_train, y_train))  # Duplicar os rótulos para corresponder ao tamanho aumentado\n",
    "\n",
    "# Converter para tensores do PyTorch\n",
    "X_train_tensor_augmented = torch.tensor(X_train_augmented, dtype=torch.float32)\n",
    "y_train_tensor_augmented = torch.tensor(y_train_augmented, dtype=torch.long)\n",
    "\n",
    "# Criar um DataLoader com os dados aumentados\n",
    "train_dataset_augmented = TensorDataset(X_train_tensor_augmented, y_train_tensor_augmented)\n",
    "train_loader_augmented = DataLoader(dataset=train_dataset_augmented, batch_size=32, shuffle=True)\n"
   ],
   "id": "2dda72ba79c0f4cb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:05.662527Z",
     "start_time": "2024-08-12T10:56:05.655783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IrisClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisClassificationModel, self).__init__()\n",
    "        self.hidden = nn.Linear(X_train.shape[1], 16)  # Camada oculta com 16 neurônios\n",
    "        self.output = nn.Linear(16, 3)  # Três classes de saída\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))  # Função de ativação ReLU na camada oculta\n",
    "        x = self.output(x)  # Saída direta (softmax será aplicado na função de perda)\n",
    "        return x\n",
    "\n",
    "model = IrisClassificationModel()"
   ],
   "id": "abb9d1f0b21dc6f5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:08.348714Z",
     "start_time": "2024-08-12T10:56:07.466270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "id": "eb9d413a02f402e2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:10.782113Z",
     "start_time": "2024-08-12T10:56:09.965965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader_augmented:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass e otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Mostrar o progresso a cada 10 épocas\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ],
   "id": "93cbaddb11f9c547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.8935\n",
      "Epoch [20/100], Loss: 0.6861\n",
      "Epoch [30/100], Loss: 0.5534\n",
      "Epoch [40/100], Loss: 0.4953\n",
      "Epoch [50/100], Loss: 0.4801\n",
      "Epoch [60/100], Loss: 0.3202\n",
      "Epoch [70/100], Loss: 0.4323\n",
      "Epoch [80/100], Loss: 0.3968\n",
      "Epoch [90/100], Loss: 0.3031\n",
      "Epoch [100/100], Loss: 0.3087\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:12.935585Z",
     "start_time": "2024-08-12T10:56:12.926026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converter os dados de teste para tensores do PyTorch\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Avaliar o modelo no conjunto de teste\n",
    "model.eval()  # Colocar o modelo em modo de avaliação\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n"
   ],
   "id": "4b863a7283bfda86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 93.33%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data Augmentation: Mixup\n",
    "================================\n",
    "\n",
    "técnica que cria novas amostras de treinamento ao combinar pares de amostras existentes, tanto em termos de features quanto de rótulos. Essa técnica é particularmente eficaz em melhorar a robustez e a generalização do modelo, especialmente em tarefas de classificação."
   ],
   "id": "e23a0a4e4685e26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Fórmulas do Mixup\n",
    "_____________________\n",
    "A ideia central do Mixup é criar novas amostras lineares interpolando entre dois exemplos aleatórios do conjunto de dados.\n",
    "* Para duas amostras $(x_i , y_i)$ e $(x_j , y_j)$, uma nova amostra $(\\tilde{x}, \\tilde{y})$ é criada da seguinte forma:\n",
    "\n",
    "$$\\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j$$\n",
    "$$\\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j$$\n",
    "\n",
    "Onde $\\lambda$ é um valor retirado de uma distribuição beta $\\beta (\\alpha , \\alpha)$, com $\\alpha$ sendo um hiperparâmetro que contra a mistura"
   ],
   "id": "8e7952c8f16fc582"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:18.206628Z",
     "start_time": "2024-08-12T10:56:18.202758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converter rótulos para one-hot encoding\n",
    "y_train_onehot = np.eye(3)[y_train]  # 3 classes no Iris\n",
    "y_test_onehot = np.eye(3)[y_test]"
   ],
   "id": "ec39f5da2d093499",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:20.362151Z",
     "start_time": "2024-08-12T10:56:20.355327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Retorna as novas amostras e rótulos após aplicação do Mixup\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_y = lam * y + (1 - lam) * y[index, :]\n",
    "\n",
    "    return mixed_x, mixed_y\n",
    "\n",
    "# Converter para tensores do PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_onehot, dtype=torch.float32)\n",
    "\n",
    "# Aplicar Mixup durante o treinamento\n",
    "class MixupDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, alpha=1.0):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.X[idx], self.y[idx]\n",
    "        return mixup_data(x.unsqueeze(0), y.unsqueeze(0), self.alpha)\n",
    "\n",
    "train_dataset_mixup = MixupDataset(X_train_tensor, y_train_tensor, alpha=0.4)\n",
    "train_loader_mixup = DataLoader(dataset=train_dataset_mixup, batch_size=32, shuffle=True)\n"
   ],
   "id": "d940b571a3c2d0cd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:25.028202Z",
     "start_time": "2024-08-12T10:56:24.056741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for (inputs, targets) in train_loader_mixup:\n",
    "        # Extrair mixup\n",
    "        inputs = inputs.squeeze(0)\n",
    "        targets = targets.squeeze(0)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass e otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Mostrar o progresso a cada 10 épocas\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ],
   "id": "332ae405f21afab8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: -0.0000\n",
      "Epoch [20/100], Loss: -0.0000\n",
      "Epoch [30/100], Loss: -0.0000\n",
      "Epoch [40/100], Loss: -0.0000\n",
      "Epoch [50/100], Loss: -0.0000\n",
      "Epoch [60/100], Loss: -0.0000\n",
      "Epoch [70/100], Loss: -0.0000\n",
      "Epoch [80/100], Loss: -0.0000\n",
      "Epoch [90/100], Loss: -0.0000\n",
      "Epoch [100/100], Loss: -0.0000\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:26.372755Z",
     "start_time": "2024-08-12T10:56:26.365250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converter os dados de teste para tensores do PyTorch\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_onehot, dtype=torch.float32)\n",
    "\n",
    "# Avaliar o modelo no conjunto de teste\n",
    "model.eval()  # Colocar o modelo em modo de avaliação\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == torch.argmax(y_test_tensor, dim=1)).sum().item() / y_test_tensor.size(0)\n",
    "    print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n"
   ],
   "id": "fd4c7827c7358080",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 93.33%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data Augmentation: SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "====================================================================\n",
    "\n",
    "Técnica de aumento de dados que gera novos exemplos sintéticos para a classe minoritária, com o objetivo de equilibrar o conjunto de dados."
   ],
   "id": "7cd2dd472b901f2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:45.375301Z",
     "start_time": "2024-08-12T10:56:45.204040Z"
    }
   },
   "cell_type": "code",
   "source": "from imblearn.over_sampling import SMOTE",
   "id": "a84ae038cc73edd7",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:46.110180Z",
     "start_time": "2024-08-12T10:56:46.100945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aplicar SMOTE para gerar novos exemplos sintéticos\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ],
   "id": "79b096a711d0963d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:46.724497Z",
     "start_time": "2024-08-12T10:56:46.720824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converter os dados aumentados para tensores do PyTorch\n",
    "X_train_tensor_smote = torch.tensor(X_train_smote, dtype=torch.float32)\n",
    "y_train_tensor_smote = torch.tensor(y_train_smote, dtype=torch.long)"
   ],
   "id": "dd945819c2524fc4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:47.193272Z",
     "start_time": "2024-08-12T10:56:47.189376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Criar um DataLoader com os dados aumentados\n",
    "train_dataset_smote = TensorDataset(X_train_tensor_smote, y_train_tensor_smote)\n",
    "train_loader_smote = DataLoader(dataset=train_dataset_smote, batch_size=32, shuffle=True)"
   ],
   "id": "67e313588c2436e0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:48.165586Z",
     "start_time": "2024-08-12T10:56:47.688781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader_smote:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass e otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Mostrar o progresso a cada 10 épocas\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ],
   "id": "af38dd0adb9e0bbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.3736\n",
      "Epoch [20/100], Loss: 0.3213\n",
      "Epoch [30/100], Loss: 0.2442\n",
      "Epoch [40/100], Loss: 0.2607\n",
      "Epoch [50/100], Loss: 0.2748\n",
      "Epoch [60/100], Loss: 0.2816\n",
      "Epoch [70/100], Loss: 0.2175\n",
      "Epoch [80/100], Loss: 0.2530\n",
      "Epoch [90/100], Loss: 0.2723\n",
      "Epoch [100/100], Loss: 0.1755\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:56:49.255920Z",
     "start_time": "2024-08-12T10:56:49.247884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converter os dados de teste para tensores do PyTorch\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Avaliar o modelo no conjunto de teste\n",
    "model.eval()  # Colocar o modelo em modo de avaliação\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n"
   ],
   "id": "d18037cccca2b29a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 100.00%\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
